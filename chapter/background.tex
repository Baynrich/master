\chapter{Background}

This chapter aims to explain the basic concepts that are the most important to the work in this thesis.



% -------------------------------------------------------
% -------------------------------------------------------
% ---------------------- Graphs -------------------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Graphs}
\subsection{Nodes and Edges}
\subsection{Directed Acyclic Graphs}




% -------------------------------------------------------
% -------------------------------------------------------
% ----------------- Machine Learning --------------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Machine Learning}

The following sections will define the term 'Machine Learning'. 
It will describe some various ways that machine learning can be applied.



\subsection{What is machine learning?}

As a term, 'Machine Learning' is a subcategory if the umbrella term 'Artificial Intelligence'.
It is proposed as an alternative to traditional algorithms. 
Machine Learning takes a set of input (training) data, and attempts to reason about some quality of the input, 
without the author of the program explicitly telling the program what quality of the input data we are interested in reasoning about.
Instead, the author provides the Machine Learning model with their optimal target for the given input,
and the model must attempt to generalize over, and design its own algorithm to fit the target.

This approach has become useful in problems where discovering the target based on the model input becomes computationally intractible,
or when the target cannot be determined as a direct consequence of the input. 
An example of such a problem is sentiment analysis of text input. 
Given the sentence 'The nice boy made fun of the kind girl', 
it is difficult to design rules which can capture the sentiment of the sentence.
On one hand, the words 'nice', 'fun', and 'kind' indicate this sentence may have a positive sentiment.
In reality, our ability to reason tells us that this is not the case, and the sentence is of a negative nature.

\textbf{Noen ord her om Classification vs Regression. Hva er forskjellen - hva er greien?}


\textbf{Machine Learning is deeply rooted in Bayesian Statistics. (Noen gode setninger om bayes. Gjerne bayes teorem? Ikke mer enn et avsnitt. Dette er en AI-oppgave, ikke en statistikk-oppgave.)}


\subsection{Linear Regression}
\label{subsection:linreg}

As a first study in the implementation of machine learning, we look at linear regression.
To explan linear regression, we view an example in a two dimensional, euclidean space.
We are given a set of input data values $ X $ . Each input value $ x_i \in X $ has a corresponding output value $ y_i \in Y $.
We can plot each input value, and its corresponding output value as coordinates.


To describe a function that best maps the given inputs with their related outputs, 
we introduce two variables $ w_0 $ and $ b_0 $. We define a function to map our input values to their corresponding output.

\[
        f(x) = w_0x + b \tag{2.1} \label{linreg}
\]

We also introduce some measure of how well our function $ \eqref{linreg} $
maps our input to our output. As we tune our function variables $ w_0 $ and $ b_0 $ to better map the input to its corresponding output,
the measure will decrease. There exists values for our variables $ w_0 $ and $ b_0 $  such that the measure defined in $ \eqref{mse} $ reaches its lowest possible value.
In this state, the function optimally maps the input data to with its corresponding output, given our function.

\[
    \mathcal{L} = \frac{1}{|X|}\sum_{i = 0}^{|X|} \big(y_i - f(x_i)\big)^2 \tag{2.2} \label{func:mse}
\]

Of course, the function defined in $ \eqref{linreg} $ is arbitrary. 
We can add or remove existing or additional function variables as we wish,
if we see that changing the function structure improves at mapping the input to the output.
This example has applied linear regression to single-dimensional inputs and single-dimensional outputs,
but the same method can be applied to inputs and outputs of higher dimensions.
The method can also be applied when the inputs and outputs are of different dimensions, by applying linear transformations.
A general formula for linear regression can therefore be 



\[
    X \in \mathbb{R}^m, \{Y, b\} \in \mathbb{R}^n, W \in \mathbb{R}^{m x n}
\]
\[
    f(X) = WX + b \tag{2.3} \label{func:linearLayer}
\]
 

\begin{figure}
    \centering
    \subfloat[\centering Input x values plotted against their corresponding y values]{{\includegraphics[width=7cm]{figures/background/linreg_input_illustrated.png} }}%
    \qquad
    \subfloat[\centering Function $ \eqref{func:mse} $ applied to function $ \eqref{linreg} $ with optimized function variables $ w_0 $ and $ b_0 $]{{\includegraphics[width=7cm]{figures/background/MSE_illustrated.png}}}
    \caption{Linear regression illustrated}%
    \label{fig:linearRegression}%
\end{figure}



\subsection{Loss}

In the previous section, a measure of how 'wrong' a mapping of input points to their corresponding output points was defined.
In the litterature, this measure is referred to as the loss function. 
The loss function defined in $ \eqref{func:mse} $ is named 'Mean Squared Error Loss', 
due to the fact that it is defined by the mean of the squared difference between our function's predictions and the true values.
The 'Mean Squared Error Loss' operator is referred to as 'MSE' for the rest of this thesis.

It is possible to define different loss functions. A loss function must be nonnegative for all input values.
Different loss functions will interpret the performance of a functions mapping of the given input data to its corresponding output in different ways.
The difference in ... can be illustrated by observing the result of applying the loss function defined in $ \eqref{func:mae} $, typically referred to as 'Mean Absolute Error Loss', shortened to 'MAE' in this thesis, as opposed using MSE.

\[ 
    \mathcal{L} = \frac{1}{|X|}\sum_{i = 0}^{|X|} \sqrt{\big(y_i - f(x_i)\big)^2} \tag{2.4} \label{func:mae} 
\]

While MSE loss sums over the squares of errors, MAE sums over the absolute errors.
Using MAE, each prediction's error term will grow linearly, given the true output value.
Using MSE, however, each prediction's error term will grow exponentially, given the true output value.
As a consequence, 'MAE' will place equal weight emphasis on each prediction's error.
MSE will place more emphasis on predictions that are further from the true output values.
Different loss functions can therefore be applied to different problems to find different solutions. 
Figure $ \eqref{fig:lossFunctions} $ illustrates an example of different loss functions being applied to the same prediction function.
The optimal function parameters were found using the MAE operator. When the MSE operator is applied to the same function parameters,
the MSE operator does not produce its lowest possible value.

\begin{figure}
    \centering
    \subfloat[\centering Loss function $ \eqref{func:mae} $ applied to function $ \eqref{linreg} $ with optimized function variables $ w_0 $ and $ b_0 $ ]{{\includegraphics[width=7cm]{figures/background/loss/mae_optimal.png}}}
    \qquad
    \subfloat[\centering Loss function $ \eqref{func:mse} $ applied to function $ \eqref{linreg} $ with function variables $ w_0 $ and $ b_0 $ optimised using  loss function $ \eqref{func:mae} $ ]{{\includegraphics[width=7cm]{figures/background/loss/mse_not_optimal.png} }}%

    \caption{Different loss functions applied to the same prediction function}%
    \label{fig:lossFunctions}%
\end{figure}

The loss functions presented so far in this section have all been focused on the regression task in machine learning applications.
For prediction tasks, a better suited loss function should be used. A common loss function for this kind of task
is the cross entropy loss function, defined in $ \eqref{func:crossentropy} $.

\[
    \mathcal{L} = - \sum_{i = 0}^{|X|}p(y_i) log\big(f(x_i)\big)
    \tag{2.5} \label{func:crossentropy}
\]



\subsection{Underfitting and overfitting}



\section{Neural Networks}




This section aims to explain the foundational concepts of what is today referred to as Neural Networks.
In section \ref{subsection:linreg}, the idea of Linear Regression was introduced.
As the foundational concepts of neural networks are introduced, it is useful to keep the idea behind Linear Regression in mind.

The explanation of Neural Networks is broken down into four parts. 
First, two curical concepts are introduced, namely 'Linear layers' and 'activation functions'. 
Then, this thesis assesses what is referred to as the 'forward pass', as this is the simplest to explain.
Finally, the 'backpropagation step' is assessed.


\subsection{Linear layers}

From our introduction of linear layers, we remember the multi-dimensional regression formula defined in equation \ref{func:linearLayer}


\subsection{Activation functions}

Activation functions are a set of non-linear functions that are utilised in neural networks to ensure non-linearity between the network layers.
An activation function may take many forms, as any non-linear mapping of input values, defined on all real numbers, will function as an activation function.



\subsubsection{Sigmoid}

The sigmoid function maps any given input value to a value between 0 and 1. 




\[
    Sigmoid(x) =  \frac{1}{1 + e^{-x}}
    \tag{2.6} \label{func:sigmoid}
\]

\subsubsection{ReLU}

The (Re)ctified (L)inear (U)nit maps any given input value to a value greater than 0, with no upper limit.

\[
    ReLU(x) =
    \begin{cases}
        0,& \text{if } x < 0\\
        x,& \text{if } x\ge 0
    \end{cases}
    \tag{2.7} \label{func:relu}
\]

\textcolor{red}{\textbf{Vis is den deriverte her og snakk om den også?}}







\subsection{Forward pass}
\subsection{Backpropagation}
Gradient descent
Stochastic gradient descent
Adam (Hvis det er denne som landes på.)



\subsection{Supervised and unsupervised learning}






















% -------------------------------------------------------
% -------------------------------------------------------
% ---------------- Graph Neural Networks ----------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Graph Neural Networks}
\subsection{The Convolution Layer}
\subsection{Permutation invariance and equivariance}
\subsection{Attention and Message Passing}




% -------------------------------------------------------
% -------------------------------------------------------
% --------------- Gated Recurrent Units -----------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Gated Recurrent Units}
\subsection{Backpropagation Through Time}