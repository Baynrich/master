\chapter{Background}

This chapter aims to explain the basic concepts that are the most important to the work in this thesis.


% -------------------------------------------------------
% -------------------------------------------------------
% ----------------- Machine Learning --------------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Machine Learning}

The following sections will define the term 'Machine Learning'. 
It will describe some various ways that machine learning can be applied.



\subsection{What is machine learning?}

As a term, 'Machine Learning' is a subcategory if the umbrella term 'Artificial Intelligence'.
It is proposed as an alternative to traditional algorithms. 
Machine Learning takes a set of input (training) data, and attempts to reason about some quality of the input, 
without the author of the program explicitly telling the program what quality of the input data we are interested in reasoning about.
Instead, the author provides the Machine Learning model with their optimal target for the given input,
and the model must attempt to generalize over, and design its own algorithm to fit the target.

This approach has become useful in problems where discovering the target based on the model input becomes computationally intractible,
or when the target cannot be determined as a direct consequence of the input. 
An example of such a problem is sentiment analysis of text input. 
Given the sentence 'The nice boy made fun of the kind girl', 
it is difficult to design rules which can capture the sentiment of the sentence.
On one hand, the words 'nice', 'fun', and 'kind' indicate this sentence may have a positive sentiment.
In reality, our ability to reason tells us that this is not the case, and the sentence is of a negative nature.

Machine Learning is deeply rooted in Bayesian Statistics. (Noen gode setninger om bayes. Gjerne bayes teorem? Ikke mer enn et avsnitt. Dette er en AI-oppgave, ikke en statistikk-oppgave.)


\subsection{Linear Regression}

As a first study in the implementation of machine learning, we look at linear regression.
To explan linear regression, we view an example in a two dimensional, euclidean space.
We are given a set of input data values $ X $ . Each input value $ x_i \in X $ has a corresponding output value $ y_i \in Y $.
We can plot each input value, and its corresponding output value as coordinates.

To describe a function that best maps the given inputs with their related outputs, 
we introduce two variables $ w_0 $ and $ b_0 $. We define a function to map our input values to their corresponding output.
\[
        f(x) = w_0x + b \tag{2.1} \label{linreg}
\]

We also introduce some measure of how well our function defined in $ \eqref{linreg} $
maps our input to our output. As we tune our function variables $ w_0 $ and $ b_0 $ to better map the input to its corresponding output,
the measure will decrease.

\[
    \mathbb{L} = \frac{1}{|X|}\sum_{i = 0}^{|X|} \big(y_i - f(x_i)\big)^2
\]









\subsubsection{Loss}
loss



\subsection{Multi Layer Perceptron}
\subsection{Neural Networks}
underfitting and overfitting?
\subsubsection{Linear Layers}
\subsubsection{Activation functions}
\subsection{Backpropagation}
\subsection{Supervised and unsupervised learning}














% -------------------------------------------------------
% -------------------------------------------------------
% ---------------------- Graphs -------------------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Graphs}
\subsection{Nodes and Edges}
\subsection{Directed Acyclic Graphs}




% -------------------------------------------------------
% -------------------------------------------------------
% ---------------- Graph Neural Networks ----------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Graph Neural Networks}
\subsection{The Convolution Layer}
\subsection{Permutation invariance and equivariance}
\subsection{Attention and Message Passing}




% -------------------------------------------------------
% -------------------------------------------------------
% --------------- Gated Recurrent Units -----------------
% -------------------------------------------------------
% -------------------------------------------------------
\section{Gated Recurrent Units}
\subsection{Backpropagation Through Time}